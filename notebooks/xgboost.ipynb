{
 "cells": [
  {
   "cell_type": "code",
   "id": "457c871f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T09:22:05.530931Z",
     "start_time": "2025-06-12T09:21:57.957890Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import talib\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.utils import resample\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "c87f5cb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T09:25:09.298952Z",
     "start_time": "2025-06-12T09:25:09.193551Z"
    }
   },
   "source": [
    "from binance.client import Client\n",
    "sys.path.append(os.path.abspath(\"..\"))  # root /PycharmProjects/MMAT\n",
    "from config.load_env import load_keys\n",
    "\n",
    "keys = load_keys()\n",
    "#print(\"Loaded keys:\", keys)\n",
    "client = Client(keys['api_key'], keys['secret_key'])"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "4699bb84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T09:25:09.549519Z",
     "start_time": "2025-06-12T09:25:09.526610Z"
    }
   },
   "source": [
    "# ================== 增强的特征工程 ==================\n",
    "class AdvancedFeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, lookback_periods=[5, 10, 20, 50]):\n",
    "        self.lookback_periods = lookback_periods\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        # 确保索引是时间序列\n",
    "        if not isinstance(df.index, pd.DatetimeIndex):\n",
    "            df.index = pd.to_datetime(df.index)\n",
    "        \n",
    "        # 基础价格特征\n",
    "        df['price_change'] = df['close'].pct_change()\n",
    "        df['high_low_ratio'] = (df['high'] - df['low']) / df['close']\n",
    "        df['close_open_ratio'] = (df['close'] - df['open']) / df['open']\n",
    "        \n",
    "        # 多时间框架特征\n",
    "        for period in [15, 30, 60]:  # 15min, 30min, 1h\n",
    "            df[f'ema_{period}'] = talib.EMA(df['close'], timeperiod=period)\n",
    "            df[f'sma_{period}'] = talib.SMA(df['close'], timeperiod=period)\n",
    "            df[f'ma_cross_{period}'] = np.where(df[f'ema_{period}'] > df[f'sma_{period}'], 1, -1)\n",
    "        \n",
    "        # 波动率特征\n",
    "        df['atr'] = talib.ATR(df['high'], df['low'], df['close'], timeperiod=14)\n",
    "        df['natr'] = talib.NATR(df['high'], df['low'], df['close'], timeperiod=14)\n",
    "        df['volatility'] = df['close'].rolling(20).std() / df['close'].rolling(20).mean()\n",
    "        \n",
    "        # 成交量特征\n",
    "        df['volume_ma'] = df['volume'].rolling(20).mean()\n",
    "        df['volume_ratio'] = df['volume'] / df['volume_ma']\n",
    "        df['obv'] = talib.OBV(df['close'], df['volume'])\n",
    "        \n",
    "        # 动量特征\n",
    "        df['rsi'] = talib.RSI(df['close'], timeperiod=14)\n",
    "        df['macd'], df['macd_signal'], _ = talib.MACD(df['close'], fastperiod=12, slowperiod=26, signalperiod=9)\n",
    "        df['macd_hist'] = df['macd'] - df['macd_signal']\n",
    "        df['stoch_k'], df['stoch_d'] = talib.STOCH(df['high'], df['low'], df['close'], fastk_period=14, slowk_period=3, slowd_period=3)\n",
    "        \n",
    "        # 高级特征\n",
    "        df['adx'] = talib.ADX(df['high'], df['low'], df['close'], timeperiod=14)\n",
    "        df['cci'] = talib.CCI(df['high'], df['low'], df['close'], timeperiod=20)\n",
    "        df['mfi'] = talib.MFI(df['high'], df['low'], df['close'], df['volume'], timeperiod=14)\n",
    "        \n",
    "        # 价格形态特征\n",
    "        df['inside_bar'] = ((df['high'] < df['high'].shift(1)) & (df['low'] > df['low'].shift(1))).astype(int)\n",
    "        df['outside_bar'] = ((df['high'] > df['high'].shift(1)) & (df['low'] < df['low'].shift(1))).astype(int)\n",
    "        \n",
    "        # 时间特征\n",
    "        df['hour'] = df.index.hour\n",
    "        df['day_of_week'] = df.index.dayofweek\n",
    "        df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "        \n",
    "        # 滞后特征\n",
    "        for lag in [1, 2, 3, 5]:\n",
    "            df[f'return_lag{lag}'] = df['price_change'].shift(lag)\n",
    "            df[f'volume_ratio_lag{lag}'] = df['volume_ratio'].shift(lag)\n",
    "        \n",
    "        # 目标编码特征（需要小心处理避免前瞻偏差）\n",
    "        # 这里省略，实际应用中需要谨慎实现\n",
    "        \n",
    "        # 删除NaN值\n",
    "        df = df.dropna()\n",
    "        \n",
    "        return df\n",
    "\n",
    "# ================== 元模型架构 ==================\n",
    "class MetaModel:\n",
    "    def __init__(self):\n",
    "        self.base_models = [\n",
    "            ('xgb_base', XGBClassifier(\n",
    "                n_estimators=300,\n",
    "                max_depth=5,\n",
    "                learning_rate=0.05,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                gamma=0.1,\n",
    "                reg_alpha=0.1,\n",
    "                reg_lambda=0.1,\n",
    "                objective='multi:softprob',\n",
    "                num_class=3,\n",
    "                use_label_encoder=False,\n",
    "                eval_metric='mlogloss'\n",
    "            )),\n",
    "            ('rf', RandomForestClassifier(\n",
    "                n_estimators=200,\n",
    "                max_depth=7,\n",
    "                class_weight='balanced',\n",
    "                random_state=42\n",
    "            )),\n",
    "            ('gbm', GradientBoostingClassifier(\n",
    "                n_estimators=150,\n",
    "                learning_rate=0.05,\n",
    "                max_depth=4,\n",
    "                random_state=42\n",
    "            ))\n",
    "        ]\n",
    "        \n",
    "        self.meta_model = XGBClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=3,\n",
    "            learning_rate=0.1,\n",
    "            objective='multi:softprob',\n",
    "            num_class=3,\n",
    "            use_label_encoder=False,\n",
    "            eval_metric='mlogloss'\n",
    "        )\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        sample_weights = compute_sample_weight(class_weight='balanced', y=y)\n",
    "        base_preds = []\n",
    "        for name, model in self.base_models:\n",
    "            model.fit(X, y, sample_weight=sample_weights)\n",
    "            preds = model.predict_proba(X)\n",
    "            base_preds.append(preds)\n",
    "\n",
    "        meta_X = np.hstack(base_preds)\n",
    "        meta_X_scaled = self.scaler.fit_transform(meta_X)\n",
    "\n",
    "        self.meta_model.fit(meta_X_scaled, y, sample_weight=sample_weights)\n",
    "        self.base_models = [(name, model) for name, model in self.base_models]\n",
    "\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        base_preds = [model.predict_proba(X) for _, model in self.base_models]\n",
    "        meta_X = np.hstack(base_preds)\n",
    "        meta_X_scaled = self.scaler.transform(meta_X)\n",
    "        return self.meta_model.predict_proba(meta_X_scaled)\n",
    "\n",
    "    def predict(self, X):\n",
    "        proba = self.predict_proba(X)\n",
    "        # 手动调整惩罚，鼓励预测为 -1, 1\n",
    "        adjusted = proba * np.array([1.2, 0.9, 1.2])  # [down, neutral, up]\n",
    "        return np.argmax(adjusted, axis=1)\n",
    "\n",
    "def balance_classes(df, label_col='label'):\n",
    "    from sklearn.utils import resample\n",
    "    df_neg = df[df[label_col] == -1]\n",
    "    df_zero = df[df[label_col] == 0]\n",
    "    df_pos = df[df[label_col] == 1]\n",
    "\n",
    "    max_len = max(len(df_neg), len(df_zero), len(df_pos))\n",
    "\n",
    "    df_neg_up = resample(df_neg, replace=True, n_samples=max_len, random_state=42)\n",
    "    df_zero_up = resample(df_zero, replace=True, n_samples=max_len, random_state=42)\n",
    "    df_pos_up = resample(df_pos, replace=True, n_samples=max_len, random_state=42)\n",
    "\n",
    "    df_balanced = pd.concat([df_neg_up, df_zero_up, df_pos_up]).sample(frac=1, random_state=42)\n",
    "    return df_balanced\n"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "16aec1c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T09:25:09.988196Z",
     "start_time": "2025-06-12T09:25:09.980007Z"
    }
   },
   "source": [
    "%%html\n",
    "<style>\n",
    "div.output_scroll {\n",
    "  height: auto !important;\n",
    "  max-height: none !important;\n",
    "}\n",
    "</style>\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<style>\n",
       "div.output_scroll {\n",
       "  height: auto !important;\n",
       "  max-height: none !important;\n",
       "}\n",
       "</style>\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "1b67a3db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T09:25:10.792597Z",
     "start_time": "2025-06-12T09:25:10.769449Z"
    }
   },
   "source": [
    "import os\n",
    "import webbrowser\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "\n",
    "def plot_pattern_results(df, patterns, symbol, max_points=2000, buffer=50, open_browser=True):\n",
    "    \"\"\"\n",
    "    Plots candlestick charts with pattern signals, ensuring no future leakage and index alignment.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame with signals already generated\n",
    "    - patterns: list of pattern names (e.g. ['BullishEngulfing', 'ThreeLineStrike'])\n",
    "    - symbol: str, name of the instrument\n",
    "    - max_points: how many points to show in the plot (excluding buffer)\n",
    "    - buffer: number of extra candles before the plotted region (e.g. to show full 3-line strikes)\n",
    "    - open_browser: whether to open HTML chart in browser\n",
    "    \"\"\"\n",
    "\n",
    "    # Determine the starting point with buffer\n",
    "    start_index = max(len(df) - max_points - buffer, 0)\n",
    "    df_plot = df.iloc[start_index:].copy()\n",
    "\n",
    "    output_dir = './plots/'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    top_patterns = ['InvertedHammer', 'ThreeLineStrike', 'HangingMan'] # Can adjust - add other candlestick pattern\n",
    "\n",
    "    for name in top_patterns:\n",
    "        signal_col = f'Signal_{name}'\n",
    "\n",
    "        if signal_col not in df_plot.columns:\n",
    "            print(f\"Warning: Signal column '{signal_col}' not found for {name}. Skipping plot.\")\n",
    "            continue\n",
    "\n",
    "        up_signals = df_plot[df_plot[signal_col] == 1]\n",
    "        down_signals = df_plot[df_plot[signal_col] == -1]\n",
    "        neutral_signals = df_plot[df_plot[signal_col] == 9]\n",
    "\n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=1,\n",
    "            shared_xaxes=True,\n",
    "            vertical_spacing=0.1,\n",
    "            subplot_titles=['Candlestick + MA', 'RSI'],\n",
    "            row_heights=[0.7, 0.3]\n",
    "        )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Candlestick(\n",
    "                x=df_plot.index,\n",
    "                open=df_plot['open'],\n",
    "                high=df_plot['high'],\n",
    "                low=df_plot['low'],\n",
    "                close=df_plot['close'],\n",
    "                name='Candlestick',\n",
    "                increasing_line_color='green',\n",
    "                decreasing_line_color='red'\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=df_plot.index, y=df_plot['MA20'], mode='lines', name='20 MA', line=dict(color='blue')),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=df_plot.index, y=df_plot['MA50'], mode='lines', name='50 MA', line=dict(color='purple')),\n",
    "            row=1, col=1\n",
    "        )\n",
    "\n",
    "        if not up_signals.empty:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=up_signals.index,\n",
    "                    y=up_signals['close'] * 1.005,\n",
    "                    mode='markers',\n",
    "                    marker=dict(symbol='triangle-up', color='green', size=10),\n",
    "                    name='Bullish Signal',\n",
    "                    text=[f'Bullish {name}' for _ in range(len(up_signals))],\n",
    "                    hoverinfo='text+x+y'\n",
    "                ),\n",
    "                row=1, col=1\n",
    "            )\n",
    "        if not down_signals.empty:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=down_signals.index,\n",
    "                    y=down_signals['close'] * 0.995,\n",
    "                    mode='markers',\n",
    "                    marker=dict(symbol='triangle-down', color='red', size=10),\n",
    "                    name='Bearish Signal',\n",
    "                    text=[f'Bearish {name}' for _ in range(len(down_signals))],\n",
    "                    hoverinfo='text+x+y'\n",
    "                ),\n",
    "                row=1, col=1\n",
    "            )\n",
    "        if not neutral_signals.empty:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=neutral_signals.index,\n",
    "                    y=neutral_signals['close'],\n",
    "                    mode='markers',\n",
    "                    marker=dict(symbol='circle', color='gray', size=8),\n",
    "                    name='Neutral Signal',\n",
    "                    text=[f'Neutral {name}' for _ in range(len(neutral_signals))],\n",
    "                    hoverinfo='text+x+y'\n",
    "                ),\n",
    "                row=1, col=1\n",
    "            )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=df_plot.index, y=df_plot['RSI'], mode='lines', name='RSI', line=dict(color='blue')),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        fig.add_hline(y=50, line_dash='dash', line_color='black', row=2, col=1)\n",
    "\n",
    "        fig.update_layout(\n",
    "            title=f'{name} Signals for {symbol} with MA and RSI',\n",
    "            xaxis_title='Time',\n",
    "            yaxis_title='Price ($)',\n",
    "            yaxis2_title='RSI',\n",
    "            xaxis_rangeslider_visible=False,\n",
    "            showlegend=True,\n",
    "            height=600,\n",
    "            template='plotly_white'\n",
    "        )\n",
    "\n",
    "        html_path = os.path.join(output_dir, f'{name}_signals.html')\n",
    "        fig.write_html(html_path)\n",
    "        print(f\"Saved plot for {name} to {html_path}\")\n",
    "\n",
    "        if open_browser:\n",
    "            abs_path = os.path.abspath(html_path)\n",
    "            webbrowser.open(f'file://{abs_path}')\n",
    "            print(f\"Opened plot for {name} in default browser\")\n"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "ec6cdce1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T09:25:12.044930Z",
     "start_time": "2025-06-12T09:25:11.075472Z"
    }
   },
   "source": [
    "# ================== signal_prediction_pipeline_updated.py ==================\n",
    "# 整合量化信号预测 pipeline，包含：\n",
    "# 1. 分位数标签 (calculate_target_quantile)\n",
    "# 2. 样本平衡 (SMOTE)\n",
    "# 3. Block Bootstrap 白检验 (block_bootstrap_pval)\n",
    "# 4. 权重缩放 (weighted_signal_evaluation)\n",
    "# 5. 蜡烛图形态特征 (integrate_candlestick_features)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import talib\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from imblearn.over_sampling import SMOTE\n",
    "# from arch.bootstrap import StationaryBootstrap\n",
    "\n",
    "# === 分位数标签函数 ===\n",
    "def calculate_target_quantile(df, future_bars=3, lower_q=0.2, upper_q=0.8):\n",
    "    future_return = df['close'].shift(-future_bars) / df['close'] - 1\n",
    "    lower = future_return.quantile(lower_q)\n",
    "    upper = future_return.quantile(upper_q)\n",
    "    conds = [\n",
    "        future_return <= lower,\n",
    "        (future_return > lower) & (future_return < upper),\n",
    "        future_return >= upper\n",
    "    ]\n",
    "    labels = [-1, 0, 1]\n",
    "    direction = np.select(conds, labels, default=0)\n",
    "    magnitude = future_return.abs()\n",
    "    return pd.DataFrame({'direction': direction, 'magnitude': magnitude}, index=df.index)\n",
    "\n",
    "# === 简单随机上采样替代 SMOTE ===\n",
    "def simple_oversample(X, y):\n",
    "    df_train = X.copy()\n",
    "    df_train['direction'] = y.values\n",
    "    majority = df_train[df_train['direction'] == 0]\n",
    "    minority = df_train[df_train['direction'] != 0]\n",
    "    minority_upsampled = resample(\n",
    "        minority,\n",
    "        replace=True,\n",
    "        n_samples=len(majority),\n",
    "        random_state=42\n",
    "    )\n",
    "    df_bal = pd.concat([majority, minority_upsampled])\n",
    "    y_bal = df_bal['direction']\n",
    "    X_bal = df_bal.drop(columns='direction')\n",
    "    return X_bal, y_bal\n",
    "\n",
    "# === 蜡烛图形态特征 ===\n",
    "def calculate_patterns(df):\n",
    "    funcs = {\n",
    "        'Hammer': talib.CDLHAMMER,\n",
    "        'InvertedHammer': talib.CDLINVERTEDHAMMER,\n",
    "        'BullishEngulfing': lambda o,h,l,c: np.where(talib.CDLENGULFING(o,h,l,c)==100,100,0),\n",
    "        'BearishEngulfing': lambda o,h,l,c: np.where(talib.CDLENGULFING(o,h,l,c)==-100,-100,0),\n",
    "        'PiercingLine': talib.CDLPIERCING,\n",
    "        'DarkCloudCover': talib.CDLDARKCLOUDCOVER,\n",
    "        'MorningStar': talib.CDLMORNINGSTAR,\n",
    "        'EveningStar': talib.CDLEVENINGSTAR,\n",
    "        'ThreeWhiteSoldiers': talib.CDL3WHITESOLDIERS,\n",
    "        'ThreeBlackCrows': talib.CDL3BLACKCROWS,\n",
    "        # ... 可根据需求增加更多形态\n",
    "    }\n",
    "    for name, fn in funcs.items():\n",
    "        df[name] = fn(df['open'].values, df['high'].values, df['low'].values, df['close'].values)\n",
    "    return df\n",
    "\n",
    "def aggregate_candlestick_signals(df):\n",
    "    bullish = ['Hammer','InvertedHammer','BullishEngulfing','PiercingLine','MorningStar','ThreeWhiteSoldiers']\n",
    "    bearish = ['BearishEngulfing','DarkCloudCover','EveningStar','ThreeBlackCrows']\n",
    "    df['bullish_score'] = df[bullish].eq(100).sum(axis=1)\n",
    "    df['bearish_score'] = df[bearish].eq(-100).sum(axis=1)\n",
    "    df['candlestick_score'] = df['bullish_score'] - df['bearish_score']\n",
    "    return df\n",
    "\n",
    "def integrate_candlestick_features(df):\n",
    "    df = calculate_patterns(df)\n",
    "    df = aggregate_candlestick_signals(df)\n",
    "    return df.dropna()\n",
    "\n",
    "# === 特征工程 ===\n",
    "class AdvancedFeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None): return self\n",
    "    def transform(self, df):\n",
    "        df = df.copy()\n",
    "        if not isinstance(df.index, pd.DatetimeIndex):\n",
    "            df.index = pd.to_datetime(df.index)\n",
    "        df['price_change'] = df['close'].pct_change()\n",
    "        df['high_low_ratio'] = (df['high'] - df['low']) / df['close']\n",
    "        df['close_open_ratio'] = (df['close'] - df['open']) / df['open']\n",
    "        df['rsi'] = talib.RSI(df['close'])\n",
    "        df['macd'], df['macd_signal'], _ = talib.MACD(df['close'])\n",
    "        df['volume_ma'] = df['volume'].rolling(20).mean()\n",
    "        df['volume_ratio'] = df['volume'] / df['volume_ma']\n",
    "        df['price_volume_corr'] = df['close'].rolling(20).corr(df['volume'])\n",
    "        df['rsi_divergence'] = df['close'] - df['rsi']\n",
    "        return df.dropna()\n",
    "\n",
    "# === Enhanced MetaModel ===\n",
    "class EnhancedMetaModel:\n",
    "    def __init__(self, n_pca=20, l1_ratio=0.5):\n",
    "        # 降维器\n",
    "        self.pca = PCA(n_components=n_pca)\n",
    "        # 基模型，增加正则化超参\n",
    "        self.base_models = [\n",
    "            ('xgb', XGBClassifier(\n",
    "                n_estimators=100,\n",
    "                reg_alpha=1.0,      # L1 正则系数\n",
    "                reg_lambda=1.0,     # L2 正则系数\n",
    "                use_label_encoder=False,\n",
    "                eval_metric='mlogloss'\n",
    "            )),\n",
    "            ('rf', RandomForestClassifier(\n",
    "                n_estimators=100,\n",
    "                max_depth=8,\n",
    "                class_weight='balanced'  # 同时保证样本不平衡时的权重\n",
    "            )),\n",
    "            ('gbm', GradientBoostingClassifier(\n",
    "                n_estimators=100,\n",
    "                learning_rate=0.1\n",
    "            ))\n",
    "        ]\n",
    "        # 元模型：带弹性网（L1+L2）\n",
    "        self.meta_model = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('pca', PCA(n_components=n_pca)),  # 再做一次降维\n",
    "            ('clf', LogisticRegression(\n",
    "                penalty='elasticnet',\n",
    "                solver='saga',\n",
    "                l1_ratio=l1_ratio,\n",
    "                C=1.0,\n",
    "                max_iter=1000,\n",
    "                class_weight='balanced'\n",
    "            ))\n",
    "        ])\n",
    "        self.scaler = StandardScaler()\n",
    "        self.encoder = LabelEncoder()\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # 先标准化+PCA\n",
    "        X_pca = self.pca.fit_transform(self.scaler.fit_transform(X))\n",
    "        y_enc = self.encoder.fit_transform(y)\n",
    "\n",
    "        # 训练基模型并收集它们的概率预测\n",
    "        meta_X = []\n",
    "        for name, model in self.base_models:\n",
    "            model.fit(X_pca, y_enc)\n",
    "            meta_X.append(model.predict_proba(X_pca))\n",
    "        meta_X = np.hstack(meta_X)\n",
    "\n",
    "        # 再用元模型做训练（管道里已经包含 scaler+pca）\n",
    "        self.meta_model.fit(X, y_enc)  # 注意：管道内部会再做一次预处理\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # 基模型输出\n",
    "        X_pca = self.pca.transform(self.scaler.transform(X))\n",
    "        meta_X = np.hstack([m.predict_proba(X_pca) for _, m in self.base_models])\n",
    "        # 元模型预测\n",
    "        preds = self.meta_model.predict(X)\n",
    "        return self.encoder.inverse_transform(preds)\n",
    "\n",
    "# === 自定义 Block Bootstrap 白检验 P-value ===\n",
    "def block_bootstrap_pval(returns, B=1000, block_len=5, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    n = len(returns)\n",
    "    indices = np.arange(n)\n",
    "    boot_means = []\n",
    "    for _ in range(B):\n",
    "        sample_idx = []\n",
    "        nb = int(np.ceil(n / block_len))\n",
    "        for _ in range(nb):\n",
    "            start = np.random.randint(0, n - block_len + 1)\n",
    "            sample_idx.extend(indices[start: start + block_len])\n",
    "        sample_idx = sample_idx[:n]\n",
    "        boot_means.append(returns[sample_idx].mean())\n",
    "    d_bar = returns.mean()\n",
    "    return np.mean([d_bar <= m for m in boot_means])\n",
    "\n",
    "# === 权重缩放评估函数 ===\n",
    "def weighted_signal_evaluation(pred_list, y_list, sharpe_list):\n",
    "    weights = np.clip(sharpe_list, 0, None)\n",
    "    weights = weights / weights.sum() if weights.sum()>0 else np.ones_like(weights)/len(weights)\n",
    "    combined = sum(w * (np.sign(y) * 0.001 * np.sign(p))\n",
    "                   for w, p, y in zip(weights, pred_list, y_list))\n",
    "    sharpe = combined.mean()/combined.std()*np.sqrt(252*24*4) if combined.std() else 0\n",
    "    acc = accuracy_score(np.concatenate(y_list), np.concatenate(pred_list))\n",
    "    return {'accuracy': acc, 'sharpe': sharpe}\n",
    "\n",
    "# === 主流程 ===\n",
    "def run_pipeline(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df.set_index('timestamp', inplace=True)\n",
    "    df = df.resample('15min').agg({'open':'first','high':'max','low':'min','close':'last','volume':'sum'}).dropna()\n",
    "    # df = df.iloc[-96*30:]\n",
    "\n",
    "    # 1. 分位数标签 & 丢弃中性样本\n",
    "    target_df = calculate_target_quantile(df, future_bars=3, lower_q=0.2, upper_q=0.8)\n",
    "    df['direction'] = target_df['direction']\n",
    "    df['magnitude'] = target_df['magnitude']\n",
    "    df = df.dropna(subset=['direction'])\n",
    "    print(\"方向标签分布:\\n\", df['direction'].value_counts(normalize=True))\n",
    "\n",
    "    # 2. 特征工程 + 蜡烛图特征\n",
    "    df_feat = AdvancedFeatureEngineer().fit_transform(df)\n",
    "    df_feat = integrate_candlestick_features(df_feat)\n",
    "\n",
    "    X = df_feat.drop(columns=['open','high','low','close','volume','direction','magnitude'])\n",
    "    y = df_feat['direction']\n",
    "\n",
    "    results, pred_list, y_list, sharpe_list = [], [], [], []\n",
    "    train_len = 5000   # 训练集用 5000 根 15min K 线\n",
    "    test_len  = 1000   # 测试集用 1000 根 15min K 线\n",
    "    step      = 1000   # 每次向前滑动 1000 根\n",
    "\n",
    "    n = len(X)\n",
    "    for start in range(0, n - train_len - test_len + 1, step):\n",
    "        train_idx = range(start, start + train_len)\n",
    "        test_idx  = range(start + train_len, start + train_len + test_len)\n",
    "\n",
    "        X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "        X_test,  y_test  = X.iloc[test_idx],  y.iloc[test_idx]\n",
    "\n",
    "        if y_train.nunique()<2:\n",
    "            print(f\"Window {start}-{start+train_len+test_len} skip: only {y_train.unique()}\")\n",
    "            continue\n",
    "\n",
    "        # SMOTE 过采样\n",
    "        X_res, y_res = SMOTE().fit_resample(X_train, y_train)\n",
    "        model = EnhancedMetaModel().fit(X_res, y_res)\n",
    "        preds = model.predict(X_test)\n",
    "\n",
    "        # 计算回报 & Sharpe\n",
    "        ret = df['close'].iloc[test_idx].pct_change().shift(-1) * np.sign(preds)\n",
    "        sharpe = ret.mean()/ret.std()*np.sqrt(252*24*4) if ret.std() else 0\n",
    "        sharpe_list.append(sharpe)\n",
    "\n",
    "        # 输出报告\n",
    "        print(f\"\\n======= Window {start}-{start+train_len-1} → Test {start+train_len}-{start+train_len+test_len-1} =======\")\n",
    "        print(classification_report(y_test, preds, digits=3))\n",
    "        print(f\"Sharpe: {sharpe:.2f} | Acc: {accuracy_score(y_test, preds):.4f}\")\n",
    "\n",
    "        results.append({'accuracy': accuracy_score(y_test, preds), 'sharpe': sharpe})\n",
    "        pred_list.append(preds)\n",
    "        y_list.append(y_test.values)\n",
    "\n",
    "    if not results:\n",
    "        print(\"❌ no valid folds\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n======= Summary =======\")\n",
    "    for i,r in enumerate(results,1): print(f\"Fold {i}: Acc={r['accuracy']:.4f}, Sharpe={r['sharpe']:.2f}\")\n",
    "\n",
    "    # 白检验\n",
    "    all_ret = np.concatenate([np.sign(y)*0.001*np.sign(p) for p,y in zip(pred_list,y_list)])\n",
    "    print(\"White Reality Check p-value:\", block_bootstrap_pval(all_ret))\n",
    "\n",
    "    # 权重缩放评估\n",
    "    w = weighted_signal_evaluation(pred_list, y_list, np.array(sharpe_list))\n",
    "    print(f\"Weighted Acc={w['accuracy']:.4f}, Sharpe={w['sharpe']:.2f}\")\n",
    "\n",
    "    # 保存最终模型\n",
    "    joblib.dump(EnhancedMetaModel().fit(X,y), \"final_model.pkl\")\n",
    "    print(\"Saved final_model.pkl\")\n",
    "\n",
    "    # 1. 生成特征 + 蜡烛图形态\n",
    "    df_feat = AdvancedFeatureEngineer().fit_transform(df)\n",
    "    df_feat = integrate_candlestick_features(df_feat)\n",
    "\n",
    "    # 2. 拷贝出来用于可视化\n",
    "    df_plot = df_feat.copy()\n",
    "\n",
    "    # 3. 计算均线和 RSI\n",
    "    df_plot['MA20'] = df_plot['close'].rolling(20).mean()\n",
    "    df_plot['MA50'] = df_plot['close'].rolling(50).mean()\n",
    "    df_plot['RSI']  = talib.RSI(df_plot['close'])\n",
    "\n",
    "    # 4. 定义你要画的形态列表\n",
    "    patterns = ['ThreeLineStrike', 'InvertedHammer', 'HangingMan']\n",
    "\n",
    "    # 5. 循环生成 Signal_ 列\n",
    "    for pat in patterns:\n",
    "        if pat not in df_plot.columns:\n",
    "            print(f\"⚠️ pattern '{pat}' not found, skipping\")\n",
    "            continue\n",
    "        sig_col = f'Signal_{pat}'\n",
    "        df_plot[sig_col] = np.where(\n",
    "            df_plot[pat] == 100,  1,\n",
    "            np.where(df_plot[pat] == -100, -1, 9)\n",
    "        )\n",
    "\n",
    "    # 6. 最后调用可视化函数\n",
    "    plot_pattern_results(\n",
    "        df = df_plot,\n",
    "        patterns = patterns,\n",
    "        symbol = 'BTCUSDT',\n",
    "        max_points = 2000,\n",
    "        buffer     = 50,\n",
    "        open_browser = True\n",
    "    )\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "009d4ba1",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-06-12T09:25:29.816392Z"
    }
   },
   "source": [
    "csv_path = \"../data/BTCUSDT_1min_2024-05-01_to_2025-05-01.csv\"\n",
    "run_pipeline(csv_path)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "方向标签分布:\n",
      " direction\n",
      " 0    0.600023\n",
      "-1    0.199989\n",
      " 1    0.199989\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "======= Window 0-4999 → Test 5000-5999 =======\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1      0.396     0.128     0.194       148\n",
      "           0      0.818     0.762     0.789       731\n",
      "           1      0.210     0.471     0.291       121\n",
      "\n",
      "    accuracy                          0.633      1000\n",
      "   macro avg      0.475     0.454     0.425      1000\n",
      "weighted avg      0.682     0.633     0.641      1000\n",
      "\n",
      "Sharpe: 0.10 | Acc: 0.6330\n",
      "\n",
      "======= Window 1000-5999 → Test 6000-6999 =======\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1      0.308     0.108     0.160       259\n",
      "           0      0.608     0.378     0.466       513\n",
      "           1      0.268     0.693     0.386       228\n",
      "\n",
      "    accuracy                          0.380      1000\n",
      "   macro avg      0.395     0.393     0.338      1000\n",
      "weighted avg      0.453     0.380     0.369      1000\n",
      "\n",
      "Sharpe: 2.20 | Acc: 0.3800\n",
      "\n",
      "======= Window 2000-6999 → Test 7000-7999 =======\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1      0.206     0.339     0.256       189\n",
      "           0      0.657     0.592     0.623       583\n",
      "           1      0.329     0.237     0.276       228\n",
      "\n",
      "    accuracy                          0.463      1000\n",
      "   macro avg      0.397     0.389     0.385      1000\n",
      "weighted avg      0.497     0.463     0.474      1000\n",
      "\n",
      "Sharpe: -0.66 | Acc: 0.4630\n",
      "\n",
      "======= Window 3000-7999 → Test 8000-8999 =======\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1      0.297     0.281     0.289       224\n",
      "           0      0.640     0.675     0.657       563\n",
      "           1      0.263     0.239     0.251       213\n",
      "\n",
      "    accuracy                          0.494      1000\n",
      "   macro avg      0.400     0.399     0.399      1000\n",
      "weighted avg      0.483     0.494     0.488      1000\n",
      "\n",
      "Sharpe: 6.93 | Acc: 0.4940\n",
      "\n",
      "======= Window 4000-8999 → Test 9000-9999 =======\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1      0.352     0.186     0.244       311\n",
      "           0      0.644     0.451     0.531       432\n",
      "           1      0.314     0.650     0.423       257\n",
      "\n",
      "    accuracy                          0.420      1000\n",
      "   macro avg      0.436     0.429     0.399      1000\n",
      "weighted avg      0.468     0.420     0.414      1000\n",
      "\n",
      "Sharpe: -1.12 | Acc: 0.4200\n",
      "\n",
      "======= Window 5000-9999 → Test 10000-10999 =======\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1      0.238     0.162     0.193       191\n",
      "           0      0.671     0.762     0.714       597\n",
      "           1      0.328     0.297     0.312       212\n",
      "\n",
      "    accuracy                          0.549      1000\n",
      "   macro avg      0.413     0.407     0.406      1000\n",
      "weighted avg      0.516     0.549     0.529      1000\n",
      "\n",
      "Sharpe: -6.28 | Acc: 0.5490\n",
      "\n",
      "======= Window 6000-10999 → Test 11000-11999 =======\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1      0.246     0.226     0.236       190\n",
      "           0      0.703     0.782     0.740       623\n",
      "           1      0.348     0.246     0.288       187\n",
      "\n",
      "    accuracy                          0.576      1000\n",
      "   macro avg      0.432     0.418     0.421      1000\n",
      "weighted avg      0.550     0.576     0.560      1000\n",
      "\n",
      "Sharpe: -3.33 | Acc: 0.5760\n",
      "\n",
      "======= Window 7000-11999 → Test 12000-12999 =======\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1      0.244     0.146     0.183       219\n",
      "           0      0.612     0.687     0.647       552\n",
      "           1      0.292     0.319     0.305       229\n",
      "\n",
      "    accuracy                          0.484      1000\n",
      "   macro avg      0.383     0.384     0.378      1000\n",
      "weighted avg      0.458     0.484     0.467      1000\n",
      "\n",
      "Sharpe: 1.26 | Acc: 0.4840\n",
      "\n",
      "======= Window 8000-12999 → Test 13000-13999 =======\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1      0.249     0.254     0.251       169\n",
      "           0      0.719     0.801     0.758       638\n",
      "           1      0.328     0.197     0.246       193\n",
      "\n",
      "    accuracy                          0.592      1000\n",
      "   macro avg      0.432     0.417     0.418      1000\n",
      "weighted avg      0.564     0.592     0.573      1000\n",
      "\n",
      "Sharpe: -1.34 | Acc: 0.5920\n",
      "\n",
      "======= Window 9000-13999 → Test 14000-14999 =======\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1      0.258     0.257     0.258       179\n",
      "           0      0.707     0.832     0.764       660\n",
      "           1      0.222     0.062     0.097       161\n",
      "\n",
      "    accuracy                          0.605      1000\n",
      "   macro avg      0.396     0.384     0.373      1000\n",
      "weighted avg      0.548     0.605     0.566      1000\n",
      "\n",
      "Sharpe: 4.49 | Acc: 0.6050\n",
      "\n",
      "======= Window 10000-14999 → Test 15000-15999 =======\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1      0.224     0.236     0.230       148\n",
      "           0      0.756     0.840     0.796       698\n",
      "           1      0.377     0.169     0.233       154\n",
      "\n",
      "    accuracy                          0.647      1000\n",
      "   macro avg      0.452     0.415     0.420      1000\n",
      "weighted avg      0.619     0.647     0.625      1000\n",
      "\n",
      "Sharpe: 0.43 | Acc: 0.6470\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab7500c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_live_klines(symbol='BTCUSDT', interval='15m', limit=5000):\n",
    "    klines = client.get_klines(symbol=symbol, interval=interval, limit=limit)\n",
    "    df = pd.DataFrame(klines, columns=[\n",
    "        'open_time','open','high','low','close','volume',\n",
    "        'close_time','quote_asset_volume','trades',\n",
    "        'taker_base','taker_quote','ignore'\n",
    "    ])\n",
    "    df['timestamp'] = pd.to_datetime(df['open_time'], unit='ms')\n",
    "    df.set_index('timestamp', inplace=True)\n",
    "    return df[['open','high','low','close','volume']].astype(float)\n",
    "\n",
    "# ===================== 主流程入口 =====================\n",
    "def run_pipeline(symbol='BTCUSDT',\n",
    "                 csv_path=None,\n",
    "                 live=False,\n",
    "                 interval='15m',\n",
    "                 history_limit=5000):\n",
    "    # 1. 数据加载\n",
    "    if live:\n",
    "        # 从 Testnet 拉取最近 history_limit 根 K 线\n",
    "        df = fetch_live_klines(symbol=symbol,\n",
    "                               interval=interval,\n",
    "                               limit=history_limit)\n",
    "    else:\n",
    "        # 从 CSV 读历史\n",
    "        df = pd.read_csv(csv_path, parse_dates=['timestamp'])\n",
    "        df.set_index('timestamp', inplace=True)\n",
    "    \n",
    "    # 2. 统一重采样到 interval\n",
    "    df = df.resample(interval).agg({\n",
    "        'open': 'first',\n",
    "        'high': 'max',\n",
    "        'low':  'min',\n",
    "        'close':'last',\n",
    "        'volume':'sum'\n",
    "    }).dropna()\n",
    "    \n",
    "    # —— 下面开始跟你原来一样的 pipeline —— #\n",
    "    \n",
    "    # 3. 分位数标签\n",
    "    target_df = calculate_target_quantile(df, future_bars=3, lower_q=0.2, upper_q=0.8)\n",
    "    df['direction'] = target_df['direction']\n",
    "    df['magnitude'] = target_df['magnitude']\n",
    "    df.dropna(subset=['direction'], inplace=True)\n",
    "    \n",
    "    # 4. 特征工程 + 蜡烛图特征\n",
    "    df_feat = AdvancedFeatureEngineer().fit_transform(df)\n",
    "    df_feat = integrate_candlestick_features(df_feat)\n",
    "    \n",
    "    X = df_feat.drop(columns=['open','high','low','close','volume','direction','magnitude'])\n",
    "    y = df_feat['direction']\n",
    "    \n",
    "    # 5. 滚动窗口回测\n",
    "    results, pred_list, y_list, sharpe_list = [], [], [], []\n",
    "    train_len, test_len, step = 5000, 1000, 1000\n",
    "    n = len(X)\n",
    "    for start in range(0, n-train_len-test_len+1, step):\n",
    "        # 5.1 划分\n",
    "        tr_idx = range(start, start+train_len)\n",
    "        te_idx = range(start+train_len, start+train_len+test_len)\n",
    "        X_tr, y_tr = X.iloc[tr_idx], y.iloc[tr_idx]\n",
    "        X_te, y_te = X.iloc[te_idx], y.iloc[te_idx]\n",
    "        if y_tr.nunique()<2:\n",
    "            continue\n",
    "        # 5.2 过采样、训练、预测\n",
    "        X_res, y_res = SMOTE().fit_resample(X_tr, y_tr)\n",
    "        model = EnhancedMetaModel().fit(X_res, y_res)\n",
    "        preds = model.predict(X_te)\n",
    "        # 5.3 计算 Sharpe/Acc\n",
    "        ret = df['close'].iloc[te_idx].pct_change().shift(-1) * np.sign(preds)\n",
    "        sharpe = ret.mean()/ret.std()*np.sqrt(252*24*4) if ret.std() else 0\n",
    "        sharpe_list.append(sharpe)\n",
    "        results.append({'accuracy':accuracy_score(y_te,preds),\n",
    "                        'sharpe':sharpe})\n",
    "        pred_list.append(preds); y_list.append(y_te.values)\n",
    "        print(f\"Fold@{start}: Acc={results[-1]['accuracy']:.3f} | Sharpe={sharpe:.2f}\")\n",
    "    \n",
    "    # 6. 汇总、白检验、权重评估、保存模型（同前）…\n",
    "    # … 你的 Summary/WhiteRC/weighted evaluation …\n",
    "    final_model = EnhancedMetaModel().fit(X,y)\n",
    "    joblib.dump(final_model, 'final_model.pkl')\n",
    "\n",
    "    # 1. 生成特征 + 蜡烛图形态\n",
    "    df_feat = AdvancedFeatureEngineer().fit_transform(df)\n",
    "    df_feat = integrate_candlestick_features(df_feat)\n",
    "\n",
    "    # 2. 拷贝出来用于可视化\n",
    "    df_plot = df_feat.copy()\n",
    "\n",
    "    # 3. 计算均线和 RSI\n",
    "    df_plot['MA20'] = df_plot['close'].rolling(20).mean()\n",
    "    df_plot['MA50'] = df_plot['close'].rolling(50).mean()\n",
    "    df_plot['RSI']  = talib.RSI(df_plot['close'])\n",
    "\n",
    "    # 4. 定义你要画的形态列表\n",
    "    patterns = ['ThreeLineStrike', 'InvertedHammer', 'HangingMan']\n",
    "\n",
    "    # 5. 循环生成 Signal_ 列\n",
    "    for pat in patterns:\n",
    "        if pat not in df_plot.columns:\n",
    "            print(f\"pattern '{pat}' not found, skipping\")\n",
    "            continue\n",
    "        sig_col = f'Signal_{pat}'\n",
    "        df_plot[sig_col] = np.where(\n",
    "            df_plot[pat] == 100,  1,\n",
    "            np.where(df_plot[pat] == -100, -1, 9)\n",
    "        )\n",
    "\n",
    "    # 6. 最后调用可视化函数\n",
    "    plot_pattern_results(\n",
    "        df = df_plot,\n",
    "        patterns = patterns,\n",
    "        symbol = 'BTCUSDT',\n",
    "        max_points = 2000,\n",
    "        buffer     = 50,\n",
    "        open_browser = True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3dd0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LiveLoop] Error: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- adx\n",
      "- atr\n",
      "- cci\n",
      "- day_of_week\n",
      "- ema_15\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- price_volume_corr\n",
      "- rsi_divergence\n",
      ", retry in 60s\n",
      "[LiveLoop] Error: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- adx\n",
      "- atr\n",
      "- cci\n",
      "- day_of_week\n",
      "- ema_15\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- price_volume_corr\n",
      "- rsi_divergence\n",
      ", retry in 60s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[94]\u001B[39m\u001B[32m, line 64\u001B[39m, in \u001B[36mlive_loop\u001B[39m\u001B[34m(symbol, interval, history_bars)\u001B[39m\n\u001B[32m     63\u001B[39m x0 = X_live.iloc[[-\u001B[32m1\u001B[39m]]                 \u001B[38;5;66;03m# shape (1, n_features)\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m64\u001B[39m pred = \u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx0\u001B[49m\u001B[43m)\u001B[49m[\u001B[32m0\u001B[39m]            \u001B[38;5;66;03m# -1 / 0 / +1\u001B[39;00m\n\u001B[32m     66\u001B[39m \u001B[38;5;66;03m# 3.4 记录预测 & 实际命中\u001B[39;00m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[73]\u001B[39m\u001B[32m, line 171\u001B[39m, in \u001B[36mEnhancedMetaModel.predict\u001B[39m\u001B[34m(self, X)\u001B[39m\n\u001B[32m    169\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[34mpredict\u001B[39m(\u001B[38;5;28mself\u001B[39m, X):\n\u001B[32m    170\u001B[39m     \u001B[38;5;66;03m# 基模型输出\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m171\u001B[39m     X_pca = \u001B[38;5;28mself\u001B[39m.pca.transform(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mscaler\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[32m    172\u001B[39m     meta_X = np.hstack([m.predict_proba(X_pca) \u001B[38;5;28;01mfor\u001B[39;00m _, m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.base_models])\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/lib/python3.12/site-packages/sklearn/utils/_set_output.py:319\u001B[39m, in \u001B[36m_wrap_method_output.<locals>.wrapped\u001B[39m\u001B[34m(self, X, *args, **kwargs)\u001B[39m\n\u001B[32m    317\u001B[39m \u001B[38;5;129m@wraps\u001B[39m(f)\n\u001B[32m    318\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[34mwrapped\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, *args, **kwargs):\n\u001B[32m--> \u001B[39m\u001B[32m319\u001B[39m     data_to_wrap = \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    320\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data_to_wrap, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[32m    321\u001B[39m         \u001B[38;5;66;03m# only wrap the first output for cross decomposition\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/lib/python3.12/site-packages/sklearn/preprocessing/_data.py:1062\u001B[39m, in \u001B[36mStandardScaler.transform\u001B[39m\u001B[34m(self, X, copy)\u001B[39m\n\u001B[32m   1061\u001B[39m copy = copy \u001B[38;5;28;01mif\u001B[39;00m copy \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m.copy\n\u001B[32m-> \u001B[39m\u001B[32m1062\u001B[39m X = \u001B[43mvalidate_data\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1063\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m   1064\u001B[39m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1065\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreset\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m   1066\u001B[39m \u001B[43m    \u001B[49m\u001B[43maccept_sparse\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcsr\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m   1067\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcopy\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcopy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1068\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mFLOAT_DTYPES\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1069\u001B[39m \u001B[43m    \u001B[49m\u001B[43mforce_writeable\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m   1070\u001B[39m \u001B[43m    \u001B[49m\u001B[43mensure_all_finite\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mallow-nan\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m   1071\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1073\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m sparse.issparse(X):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/lib/python3.12/site-packages/sklearn/utils/validation.py:2919\u001B[39m, in \u001B[36mvalidate_data\u001B[39m\u001B[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001B[39m\n\u001B[32m   2845\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Validate input data and set or check feature names and counts of the input.\u001B[39;00m\n\u001B[32m   2846\u001B[39m \n\u001B[32m   2847\u001B[39m \u001B[33;03mThis helper function should be used in an estimator that requires input\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   2917\u001B[39m \u001B[33;03m    validated.\u001B[39;00m\n\u001B[32m   2918\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m2919\u001B[39m \u001B[43m_check_feature_names\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_estimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreset\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2920\u001B[39m tags = get_tags(_estimator)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/lib/python3.12/site-packages/sklearn/utils/validation.py:2777\u001B[39m, in \u001B[36m_check_feature_names\u001B[39m\u001B[34m(estimator, X, reset)\u001B[39m\n\u001B[32m   2775\u001B[39m     message += \u001B[33m\"\u001B[39m\u001B[33mFeature names must be in the same order as they were in fit.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m-> \u001B[39m\u001B[32m2777\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(message)\n",
      "\u001B[31mValueError\u001B[39m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- adx\n- atr\n- cci\n- day_of_week\n- ema_15\n- ...\nFeature names seen at fit time, yet now missing:\n- price_volume_corr\n- rsi_divergence\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[94]\u001B[39m\u001B[32m, line 114\u001B[39m\n\u001B[32m    111\u001B[39m             time.sleep(\u001B[32m60\u001B[39m)\n\u001B[32m    113\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[34m__name__\u001B[39m==\u001B[33m\"\u001B[39m\u001B[33m__main__\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m114\u001B[39m     \u001B[43mlive_loop\u001B[49m\u001B[43m(\u001B[49m\u001B[43msymbol\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mBTCUSDT\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minterval\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m15m\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhistory_bars\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m500\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[94]\u001B[39m\u001B[32m, line 111\u001B[39m, in \u001B[36mlive_loop\u001B[39m\u001B[34m(symbol, interval, history_bars)\u001B[39m\n\u001B[32m    109\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    110\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m[LiveLoop] Error: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m, retry in 60s\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m111\u001B[39m     \u001B[43mtime\u001B[49m\u001B[43m.\u001B[49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m60\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from binance.client import Client\n",
    "import joblib\n",
    "import talib\n",
    "\n",
    "from prediction_logger import PredictionLogger\n",
    "from signal_logger import SignalHistoryLogger\n",
    "\n",
    "# —— Step 0: 初始化 Testnet 客户端 & 日志器 & 离线加载模型 —— #\n",
    "from binance.client import Client\n",
    "# sys.path.append(os.path.abspath(\"..\"))  # root /PycharmProjects/MMAT\n",
    "from config.load_env import load_keys\n",
    "\n",
    "keys = load_keys()\n",
    "#print(\"Loaded keys:\", keys)\n",
    "client = Client(keys['api_key'], keys['secret_key'])\n",
    "\n",
    "# 载入提前离线训练并保存好的模型\n",
    "model = joblib.load(\"final_model.pkl\")\n",
    "\n",
    "pred_logger = PredictionLogger()\n",
    "sig_logger  = SignalHistoryLogger(\"signal_history.csv\")\n",
    "\n",
    "# 确保程序退出时会把日志落盘\n",
    "import atexit\n",
    "atexit.register(lambda: pred_logger.save_to_csv(\"prediction_log.csv\"))\n",
    "atexit.register(lambda: sig_logger.save_to_csv())\n",
    "\n",
    "# —— Step 1: 实时拉取 OHLCV —— #\n",
    "def fetch_ohlcv(symbol=\"BTCUSDT\", interval=\"15m\", limit=500):\n",
    "    klines = client.get_klines(symbol=symbol, interval=interval, limit=limit)\n",
    "    df = pd.DataFrame(klines, columns=[\n",
    "        'open_time','open','high','low','close','volume',\n",
    "        'close_time','quote_vol','trades','tb_base','tb_quote','ignore'])\n",
    "    df['timestamp'] = pd.to_datetime(df['open_time'], unit='ms')\n",
    "    df.set_index('timestamp', inplace=True)\n",
    "    return df[['open','high','low','close','volume']].astype(float)\n",
    "\n",
    "# —— Step 2: 画图函数（复用你已有的 plot_pattern_results） —— #\n",
    "# from your_viz_module import plot_pattern_results\n",
    "\n",
    "# —— Step 3: 实时主循环 —— #\n",
    "def live_loop(symbol=\"BTCUSDT\",\n",
    "              interval=\"15m\",\n",
    "              history_bars=500):\n",
    "    last_close = None\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # 3.1 获取最新数据\n",
    "            df = fetch_ohlcv(symbol, interval, history_bars)\n",
    "            \n",
    "            # 3.2 特征工程（只需工程出模型需要的 X）\n",
    "            df_feat = AdvancedFeatureEngineer().fit_transform(df)\n",
    "            df_feat = integrate_candlestick_features(df_feat)\n",
    "            X_live = df_feat.drop(columns=['open','high','low','close','volume','direction','magnitude'],\n",
    "                                  errors='ignore')\n",
    "            \n",
    "            # 3.3 对最新一根bar 做预测\n",
    "            x0 = X_live.iloc[[-1]]                 # shape (1, n_features)\n",
    "            pred = model.predict(x0)[0]            # -1 / 0 / +1\n",
    "            \n",
    "            # 3.4 记录预测 & 实际命中\n",
    "            now = X_live.index[-1]\n",
    "            close_now  = df['close'].iloc[-1]\n",
    "            if last_close is not None:\n",
    "                # UP / DOWN / HOLD\n",
    "                label = \"UP\" if pred==1 else (\"DOWN\" if pred==-1 else \"HOLD\")\n",
    "                pred_logger.record_prediction(now, label, close_now, last_close)\n",
    "            \n",
    "            # 3.5 如果是真实信号（非 0），落地到 signal_history\n",
    "            if pred == 1:\n",
    "                sig_logger.add_signal('bullish', now, close_now, trigger=f\"pred==1\")\n",
    "            elif pred == -1:\n",
    "                sig_logger.add_signal('bearish', now, close_now, trigger=f\"pred==-1\")\n",
    "            \n",
    "            last_close = close_now\n",
    "            \n",
    "            # 3.6 可视化最近信号（可选）\n",
    "            df_plot = df_feat.copy()\n",
    "            # 计算辅助指标\n",
    "            df_plot['MA20'] = df_plot['close'].rolling(20).mean()\n",
    "            df_plot['MA50'] = df_plot['close'].rolling(50).mean()\n",
    "            df_plot['RSI']  = talib.RSI(df_plot['close'])\n",
    "            # 生成 Signal_ 列\n",
    "            patterns = ['ThreeLineStrike','InvertedHammer','HangingMan']\n",
    "            for pat in patterns:\n",
    "                if pat in df_plot:\n",
    "                    df_plot[f\"Signal_{pat}\"] = np.where(\n",
    "                        df_plot[pat]==100, 1,\n",
    "                        np.where(df_plot[pat]==-100, -1, 9)\n",
    "                    )\n",
    "            # 调用你写好的画图函数，只看最后 200 根 K 线\n",
    "            plot_pattern_results(\n",
    "                df=df_plot,\n",
    "                patterns=patterns,\n",
    "                symbol=symbol,\n",
    "                max_points=200,\n",
    "                buffer=20,\n",
    "                open_browser=False  # 如果太频繁就关掉自动打开\n",
    "            )\n",
    "            \n",
    "            print(f\"[{now}] Pred={pred}, HitRate={pred_logger.get_hit_rate():.3f}\")\n",
    "            time.sleep(10)  # 每 5 分钟跑一次\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[LiveLoop] Error: {e}, retry in 60s\")\n",
    "            time.sleep(60)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    live_loop(symbol=\"BTCUSDT\", interval=\"15m\", history_bars=500)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
